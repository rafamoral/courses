---
title: "Time Series and Forecasting -- Practical 4"
author: ""
date: ""
output: html_document
self-contained: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(fpp3)
```

## Exercise 1

The population of Switzerland from 1960 to 2017 is in data set `global_economy`.

**(a)** Produce a time plot of the data.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

global_economy %>%
  filter(Country == "Switzerland") %>%
  autoplot(Population)
```

**(b)** You decide to fit the following model to the series:
$$y_t = c + y_{t-1} + \phi_1 (y_{t-1} - y_{t-2}) + \phi_2 (y_{t-2} - y_{t-3}) + \phi_3( y_{t-3} - y_{t-4}) + \varepsilon_t$$
where $y_t$ is the Population in year $t$ and $\varepsilon_t$ is a white noise series.
What sort of ARIMA model is this (i.e., what are $p$, $d$, and $q$)?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

# This is an ARIMA(3,1,0) model with drift.
```

**(c)** Explain why this model was chosen using the ACF and PACF of the differenced series.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

global_economy %>%
  filter(Country == "Switzerland") %>%
  ACF(difference(Population)) %>%
  autoplot

global_economy %>%
  filter(Country == "Switzerland") %>%
  PACF(difference(Population)) %>%
  autoplot

There is a significant partial autocorrelation at lag 3 for the differenced series.
```

**(d)** The last five values of the series are given below.

```{r swisspop, echo=FALSE, warning=FALSE, message=FALSE}
swiss_pop <- global_economy %>%
  filter(Country == "Switzerland") %>%
  tail(5) %>%
  select(Year, Population) %>%
  mutate(Population = Population / 1e6)
tab <- as.data.frame(matrix(c(NA, swiss_pop$Population), nrow = 1))
colnames(tab) <- c("Year", swiss_pop$Year)
tab[1, 1] <- "Population (millions)"
tab %>%
  knitr::kable(digits = 2)
fit <- global_economy %>%
  filter(Country == "Switzerland") %>%
  mutate(Population = Population / 1e6) %>%
  model(ARIMA(Population ~ 1 + pdq(3, 1, 0)))
coef <- rlang::set_names(tidy(fit)$estimate, tidy(fit)$term)
phi1 <- coef["ar1"]
phi2 <- coef["ar2"]
phi3 <- coef["ar3"]
intercept <- coef["constant"]
fc1 <- intercept + swiss_pop$Population[5] + sum(c(phi1,phi2,phi3)*diff(swiss_pop$Population)[4:2])
fc2 <- intercept + fc1 + sum(c(phi1,phi2,phi3)*diff(c(swiss_pop$Population,fc1))[5:3])
fc3 <- intercept + fc2 + sum(c(phi1,phi2,phi3)*diff(c(swiss_pop$Population,fc1,fc2))[6:4])
```

The estimated parameters are
$c = `r sprintf("%.4f",intercept)`$,
$\phi_1 = `r sprintf("%.2f", phi1)`$,
$\phi_2 = `r sprintf("%.2f", phi2)`$, and
$\phi_3 = `r sprintf("%.2f", phi3)`$.
Without using the `forecast` function, calculate forecasts for the next three years (2018--2020).

*Answer*:

\begin{align*}
  \hat{y}_{T+1|T} & = `r sprintf("%.4f",intercept)` +
  `r sprintf("%.2f",swiss_pop$Population[5])`+ 
    `r sprintf("%.2f", phi1)`* (`r sprintf("%.2f",swiss_pop$Population[5])` - `r sprintf("%.2f",swiss_pop$Population[4])`)
    `r sprintf("%.2f", phi2)`* (`r sprintf("%.2f",swiss_pop$Population[4])` - `r sprintf("%.2f",swiss_pop$Population[3])`) +
    `r sprintf("%.2f", phi3)`* (`r sprintf("%.2f",swiss_pop$Population[3])` - `r sprintf("%.2f",swiss_pop$Population[2])`) =
    `r sprintf("%.2f", fc1)` \\
  \hat{y}_{T+2|T} & = `r sprintf("%.4f",intercept)` +
  `r sprintf("%.2f",fc1)`+ 
    `r sprintf("%.2f", phi1)`* (`r sprintf("%.2f",fc1)` - `r sprintf("%.2f",swiss_pop$Population[5])`)
    `r sprintf("%.2f", phi2)`* (`r sprintf("%.2f",swiss_pop$Population[5])` - `r sprintf("%.2f",swiss_pop$Population[4])`) +
    `r sprintf("%.2f", phi3)`* (`r sprintf("%.2f",swiss_pop$Population[4])` - `r sprintf("%.2f",swiss_pop$Population[3])`) =
    `r sprintf("%.2f", fc2)` \\
  \hat{y}_{T+3|T} & = `r sprintf("%.4f",intercept)` +
  `r sprintf("%.2f",fc2)`+ 
    `r sprintf("%.2f", phi1)`* (`r sprintf("%.2f",fc2)` - `r sprintf("%.2f",fc1)`)
    `r sprintf("%.2f", phi2)`* (`r sprintf("%.2f",fc1)` - `r sprintf("%.2f",swiss_pop$Population[5])`) +
    `r sprintf("%.2f", phi3)`* (`r sprintf("%.2f",swiss_pop$Population[5])` - `r sprintf("%.2f",swiss_pop$Population[4])`) =
    `r sprintf("%.2f", fc3)` \\
\end{align*}


**(e)** Now fit the model in R and obtain the forecasts from the same model. How are they different from yours? Why?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

global_economy %>%
  filter(Country == "Switzerland") %>%
  mutate(Population = Population / 1e6) %>%
  model(ARIMA(Population ~ 1 + pdq(3, 1, 0))) %>%
  forecast(h=3)

# Any differences will be due to rounding errors.
```

## Exercise 2

Consider the number of Snowshoe Hare furs traded by the Hudson Bay Company between 1845 and 1935 (data set `pelt`).

**(a)** Produce a time plot of the time series.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

pelt %>%
  autoplot(Hare)
```

**(b)** Assume you decide to fit the following model:

$$ y_t = c + \phi_1 y_{t-1} + \phi_2 y_{t-2} + \phi_3 y_{t-3} + \phi_4 y_{t-4} + \varepsilon_t,$$

where $\varepsilon_t$ is a white noise series. What sort of ARIMA model is this (i.e., what are $p$, $d$, and $q$)?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

# This is an ARIMA(4,0,0) model with drift.
```

**(c)** By examining the ACF and PACF of the data, explain why this model is appropriate.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

pelt %>%
  ACF(Hare) %>%
  autoplot

pelt %>%
  PACF(Hare) %>%
  autoplot

# There is a significant partial autocorrelation at lag 4.
```

**(d)** The last five values of the series are given below:

```{r hares, echo=FALSE, warning=FALSE, message=FALSE}
pelt_table <- pelt %>%
  tail(5) %>%
  select(Year, Hare)
tab <- as.data.frame(matrix(c(NA, pelt_table$Hare), nrow = 1))
colnames(tab) <- c("Year", pelt_table$Year)
tab[1, 1] <- "Number of hare pelts"
tab %>%
  knitr::kable()

fit <- pelt %>% model(ARIMA(Hare ~ pdq(4, 0, 0)))
coef <- rlang::set_names(tidy(fit)$estimate, tidy(fit)$term)
constant <- coef["constant"]
phi1 <- coef["ar1"]
phi2 <- coef["ar2"]
phi3 <- coef["ar3"]
phi4 <- coef["ar4"]
fc1 <- constant + sum(c(phi1,phi2,phi3,phi4)*pelt_table$Hare[5:2])
fc2 <- constant + sum(c(phi1,phi2,phi3,phi4)*c(fc1,pelt_table$Hare[5:3]))
fc3 <- constant + sum(c(phi1,phi2,phi3,phi4)*c(fc2,fc1,pelt_table$Hare[5:4]))
```

The estimated parameters are
$c = `r sprintf("%.0f",constant)`$,
$\phi_1 = `r sprintf("%.2f", phi1)`$,
$\phi_2 = `r sprintf("%.2f", phi2)`$,
$\phi_3 = `r sprintf("%.2f", phi3)`$, and
$\phi_4 = `r sprintf("%.2f", phi4)`$.
Without using the `forecast` function, calculate forecasts for the next three years (1936--1939).

*Answer:*

\begin{align*}
  \hat{y}_{T+1|T} & = `r sprintf("%.0f",constant)` +
    `r sprintf("%.2f", phi1)`* `r sprintf("%.0f",pelt_table$Hare[5])`
    `r sprintf("%.2f", phi2)`* `r sprintf("%.0f",pelt_table$Hare[4])`
    `r sprintf("%.2f", phi3)`* `r sprintf("%.0f",pelt_table$Hare[3])`
    `r sprintf("%.2f", phi4)`* `r sprintf("%.0f",pelt_table$Hare[2])` =
    `r sprintf("%.2f", fc1)` \\
  \hat{y}_{T+2|T} & = `r sprintf("%.0f",constant)` +
    `r sprintf("%.2f", phi1)`* `r sprintf("%.2f",fc1)`
    `r sprintf("%.2f", phi2)`* `r sprintf("%.0f",pelt_table$Hare[5])`
    `r sprintf("%.2f", phi3)`* `r sprintf("%.0f",pelt_table$Hare[4])`
    `r sprintf("%.2f", phi4)`* `r sprintf("%.0f",pelt_table$Hare[3])` =
    `r sprintf("%.2f", fc2)` \\
  \hat{y}_{T+3|T} & = `r sprintf("%.0f",constant)` +
    `r sprintf("%.2f", phi1)`* `r sprintf("%.2f",fc2)`
    `r sprintf("%.2f", phi2)`* `r sprintf("%.2f",fc1)`
    `r sprintf("%.2f", phi3)`* `r sprintf("%.0f",pelt_table$Hare[5])`
    `r sprintf("%.2f", phi4)`* `r sprintf("%.0f",pelt_table$Hare[4])` =
    `r sprintf("%.2f", fc3)` \\
\end{align*}

**(e)** Now fit the model in R and obtain the forecasts using `forecast`. How are they different from yours? Why?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

pelt %>%
  model(ARIMA(Hare ~ pdq(4, 0, 0))) %>%
  forecast(h=3)

# Any differences will be due to rounding errors.
```

## Exercise 3

Choose a series from `us_employment`, the total employment in different industries in the United States.

**(a)** Produce an STL decomposition of the data and describe the trend and seasonality.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

leisure <- us_employment %>%
  filter(Title == "Leisure and Hospitality")
leisure %>%
  autoplot(Employed)

# The sudden change in the seasonal pattern is probably due to some change in the definition of who is counted in this group. So our STL decomposition will need to have a small seasonal window to handle that. In addition, the variation changes a little as the level increases, so we will also use a square root transformation. 

leisure %>%
  model(STL(sqrt(Employed) ~ season(window=7))) %>%
  components %>%
  autoplot()

# With such a long series, it is not surprising to see the seasonality change a lot over time. The seasonal pattern changed in the 1990s to what is is now. The period of change was rapid, and the seasonal component hasn't fully captured the change, leading to some seasonality ending up in the remainder series. The trend is increasing.
```

**(b)** Do the data need transforming? If so, find a suitable transformation.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

# Yes. A square root did ok -- the remainder series is relatively homoscedastic. No transformation or log transformations led to the remainder series appearing to be heteroscedastic.
```

**(c)** Are the data stationary? If not, find an appropriate differencing which yields stationary data.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

leisure %>%
  autoplot(sqrt(Employed) %>% difference(lag=12) %>% difference())

# The double differenced logged data is close to stationary, although the variance has decreased over time.
```

**(d)** Identify a couple of ARIMA models that might be useful in describing the time series. Which of your models is the best according to their AICc values?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

leisure %>%
  gg_tsdisplay(sqrt(Employed) %>% difference(lag=12) %>% difference(), plot_type="partial")

# This suggests that an ARIMA(2,1,0)(0,1,1) would be a good start.
# An alternative would be an ARIMA(0,1,2)(0,1,1).

fit <- leisure %>%
  model(
    arima210011 = ARIMA(sqrt(Employed) ~ pdq(2,1,0) + PDQ(0,1,1)),
    arima012011 = ARIMA(sqrt(Employed) ~ pdq(0,1,2) + PDQ(0,1,1))
  )
glance(fit)

#The ARIMA(2,1,0)(0,1,1) model is better.
```

**(e)** Estimate the parameters of your best model and do diagnostic testing on the residuals. Do the residuals resemble white noise? If not, try to find another ARIMA model which fits better.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

fit %>%
  select(arima210011) %>%
  gg_tsresiduals()

# The tails of the residual distribution are too long, and there is significant autocorrelation at lag 11, as well as some smaller significant spikes elsewhere.

fit <- leisure %>%
  model(
    arima210011 = ARIMA(sqrt(Employed) ~ pdq(2,1,0) + PDQ(0,1,1)),
    arima012011 = ARIMA(sqrt(Employed) ~ pdq(0,1,2) + PDQ(0,1,1)),
    auto = ARIMA(sqrt(Employed))
  )
glance(fit)
fit %>% select(auto) %>% report()

# The automatically selected ARIMA(2,1,2)(0,1,1) model is better than either of my selections.

fit %>%
  select(auto) %>%
  gg_tsresiduals()

# The residuals look better, although there is still a significant spike at lag 11.
```

## Exercise 4

Consider `aus_arrivals`, the quarterly number of international visitors to Australia from several countries for the period 1981 Q1 -- 2012 Q3.

**(a)** Select one country and describe the time plot.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

aus_arrivals %>%
  filter(Origin == "Japan") %>%
  autoplot(Arrivals)

# There is an increasing trend to about 1996, and slowly decreasing thereafter.
# The seasonal shape has changed considerably over time.
```

**(b)** Use differencing to obtain stationary data.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

aus_arrivals %>%
  filter(Origin == "Japan") %>%
  gg_tsdisplay(Arrivals %>% difference(lag=4) %>% difference(), plot_type = "partial")
```

**(c)** What can you learn from the ACF graph of the differenced data?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

# The non-seasonal lags suggest an MA(1) component.
# The seasonal lags suggest a seasonal MA(1) component
```

**(d)** What can you learn from the PACF graph of the differenced data?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

# The non-seasonal lags might suggest an AR(1) component. But the lag 2 spike in the PACF is larger than the lag 2 spike in the ACF. So an MA(1) is probably better.
# The seasonal lags show geometric decay which is consistent with a seasonal MA(1).
```

**(e)** What model do these graphs suggest?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

# The suggested model is an ARIMA(0,1,1)(0,1,1).
```

**(f)** Does `ARIMA()` give the same model that you chose? If not, which model do you think is better?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

aus_arrivals %>%
  filter(Origin == "Japan") %>%
  model(ARIMA(Arrivals))

# The resulting model has an additional seasonal AR(1) component compared to what I guessed. We can compare the two models based on the AICc statistic:

aus_arrivals %>%
  filter(Origin == "Japan") %>%
  model(
    guess = ARIMA(Arrivals ~ pdq(0,1,1) + PDQ(0,1,1)),
    auto = ARIMA(Arrivals)
  ) %>%
  glance()

# The automatic model is only slightly better than my guess based on the AICc statistic.
```

## Exercise 5

Simulate and plot some data from simple ARIMA models.

**(a)** Use the following R code to generate data from an AR(1) model with $\phi_{1} = 0.6$ and $\sigma^2=1$. The process starts with $y_1=0$.

```{r ex6a}
ar1 <- function(phi, n = 100L) {
  y <- numeric(n)
  e <- rnorm(n)
  for (i in 2:n) {
    y[i] <- phi * y[i - 1] + e[i]
  }
  tsibble(idx = seq_len(n), y = y, index = idx)
}
```

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

ar1(0.6)
```

**(b)** Produce a time plot for the series. How does the plot change as you change $\phi_1$?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

# Some examples of changing phi1

ar1(0.6) %>% autoplot(y)
ar1(0.95) %>% autoplot(y)
ar1(0.05) %>% autoplot(y)
ar1(-0.65) %>% autoplot(y)
```

**(c)** Write your own code to generate data from an MA(1) model with $\theta_{1}  =  0.6$ and $\sigma^2=1$.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

ma1 <- function(theta, n = 100L) {
  y <- numeric(n)
  e <- rnorm(n)
  for (i in 2:n) {
    y[i] <- theta * e[i - 1] + e[i]
  }
  tsibble(idx = seq_len(n), y = y, index = idx)
}
```

**(d)** Produce a time plot for the series. How does the plot change as you change $\theta_1$?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

ma1(0.6) %>% autoplot(y)
ma1(0.95) %>% autoplot(y)
ma1(0.05) %>% autoplot(y)
ma1(-0.65) %>% autoplot(y)
```

**(e)** Generate data from an ARMA(1,1) model with $\phi_{1} = 0.6$, $\theta_{1}  = 0.6$ and $\sigma^2=1$. Graph the data.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

arma11 <- function(phi, theta, n = 100) {
  y <- numeric(n)
  e <- rnorm(n)
  for (i in 2:n) {
    y[i] <- phi * y[i - 1] + theta * e[i - 1] + e[i]
  }
  tsibble(idx = seq_len(n), y = y, index = idx)
}

arma11(0.6, 0.6) %>% autoplot(y)
```

**(f)** Generate data from an AR(2) model with $\phi_{1} =-0.8$, $\phi_{2} = 0.3$ and $\sigma^2=1$. (Note that these parameters will give a non-stationary series.) Graph the data.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

ar2 <- function(phi1, phi2, n = 100) {
  y <- numeric(n)
  e <- rnorm(n)
  for (i in 3:n) {
    y[i] <- phi1 * y[i - 1] + phi2 * y[i - 2] + e[i]
  }
  tsibble(idx = seq_len(n), y = y, index = idx)
}

ar2(-0.8, 0.3) %>% autoplot(y)
```

## Exercise 6

Consider `aus_airpassengers`, the total number of passengers (in millions) from Australian air carriers for the period 1970-2011.

**(a)** Use `ARIMA()` to find an appropriate ARIMA model. What model was selected. Check that the residuals look like white noise. Plot forecasts for the next 10 periods.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

aus_airpassengers %>% autoplot(Passengers)
fit <- aus_airpassengers %>%
  model(arima = ARIMA(Passengers))
report(fit)
fit %>% gg_tsresiduals()
fit %>% forecast(h = 10) %>% autoplot(aus_airpassengers)
```

**(b)** Plot forecasts from an ARIMA(0,1,0) model with drift and compare these to part a.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

aus_airpassengers %>%
  model(arima = ARIMA(Passengers ~ 1 + pdq(0,1,0))) %>%
  forecast(h = 10) %>%
  autoplot(aus_airpassengers)

# Both containing increasing trends, but the ARIMA(0,2,1) model has an implicit trend due to the double-differencing, while the ARIMA(0,1,0) with drift models the trend directly via the trend coefficient. The intervals are narrower when there are fewer differences.
```

**(c)** Plot forecasts from an ARIMA(2,1,2) model with drift and compare these to part b. Remove the constant and see what happens.

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

aus_airpassengers %>%
  model(arima = ARIMA(Passengers ~ 1 + pdq(2,1,2))) %>%
  forecast(h = 10) %>%
  autoplot(aus_airpassengers)
aus_airpassengers %>%
  model(arima = ARIMA(Passengers ~ 0 + pdq(2,1,2)))

# There is little difference between ARIMA(2,1,2) with drift and ARIMA(0,1,0) with drift. Removing the constant causes an error because the model cannot be estimated.
```

**(d)** Plot forecasts from an ARIMA(0,2,1) model with a constant. What happens?

```{r, eval = FALSE}
#| code-fold: true
#| code-summary: "Show Answer"

aus_airpassengers %>%
  model(arima = ARIMA(Passengers ~ 1 + pdq(0,2,1))) %>%
  forecast(h = 10) %>%
  autoplot(aus_airpassengers)

# The forecast trend is now quadratic, and there is a warning that this is generally a bad idea.
```